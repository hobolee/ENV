{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_colab2.ipynb","provenance":[],"mount_file_id":"1GdhyCcLQzA4HmhvX9uAi3_LejmGGLyWD","authorship_tag":"ABX9TyNOZvT4PQZf1w+e8cAo8jh3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"YpBL3lnRJSyB","executionInfo":{"status":"ok","timestamp":1652948004782,"user_tz":-480,"elapsed":269,"user":{"displayName":"haobo li","userId":"11080054968865471708"}}},"outputs":[],"source":["import torch\n","from torch import nn, optim\n","from collections import OrderedDict\n","import random\n","import time\n","import numpy as np"]},{"cell_type":"code","source":["class MyFlattenLayer(torch.nn.Module):\n","    def __init__(self):\n","        super(MyFlattenLayer, self).__init__()\n","    def forward(self, x): # x shape: (batch, *, *, ...)\n","        x = x.transpose(1, 2)\n","        return x.reshape(x.shape[0], x.shape[1], -1)\n","\n","\n","class MySequential(nn.Module):\n","    def __init__(self, *args):\n","        super(MySequential, self).__init__()\n","        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict\n","            for key, module in args[0].items():\n","                self.add_module(key, module)  # add_module方法会将module添加进self._modules(一个OrderedDict)\n","        else:  # 传入的是一些Module\n","            for idx, module in enumerate(args):\n","                self.add_module(str(idx), module)\n","\n","    def forward(self, input):\n","        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成\n","        for module in self._modules.values():\n","            if type(module) is torch.nn.modules.rnn.LSTM:\n","                # input = input.view(-1, 30, 84*4)\n","                input, (h_n, c_n) = module(input)\n","                input = input[:, -1, :]\n","                # print('lstm', input.size())\n","            else:\n","                input = module(input)\n","                # print('other', input.size())\n","        return input"],"metadata":{"id":"CvZBZOCMJanB","executionInfo":{"status":"ok","timestamp":1652948005800,"user_tz":-480,"elapsed":2,"user":{"displayName":"haobo li","userId":"11080054968865471708"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["net = MySequential(\n","            nn.Conv3d(1, 16, (5, 7, 7), stride=1, padding=0), # in_channels, out_channels, kernel_size\n","            nn.BatchNorm3d(16),\n","            nn.ReLU(),\n","            nn.MaxPool3d(2, 2), # kernel_size, stride\n","            # nn.Conv3d(16, 32, (5, 7, 7), stride=1, padding=0),\n","            # nn.BatchNorm3d(32),\n","            # nn.ReLU(),\n","            # nn.MaxPool3d(2, 2),\n","            # nn.Conv3d(32, 64, (5, 7, 7), stride=1, padding=0),\n","            # nn.BatchNorm3d(64),\n","            # nn.ReLU(),\n","            # nn.MaxPool3d(2, 2),\n","            MyFlattenLayer(),\n","            nn.Linear(16*7*7, 512),\n","            nn.BatchNorm1d(47, 512),\n","            nn.ReLU(),\n","            # nn.Linear(1024, 256),\n","            # nn.BatchNorm1d(11, 256),\n","            # nn.ReLU(),\n","            #nn.Linear(84, 10)\n","            nn.LSTM(512, 256, num_layers=1, batch_first=True),\n","            # nn.Linear(512, 64),\n","            # nn.BatchNorm1d(64),\n","            # nn.ReLU(),\n","            nn.Linear(256, 128),\n","            nn.ReLU()\n","        )\n","\n","net2 = nn.Sequential(\n","    nn.Linear(4, 1)\n",")\n","\n","net3 = nn.Sequential(\n","    nn.Linear(129, 64),\n","    # nn.BatchNorm1d(1, 64),\n","    nn.Linear(64, 1)\n",")"],"metadata":{"id":"3s1GF1EzJybd","executionInfo":{"status":"ok","timestamp":1652948006248,"user_tz":-480,"elapsed":2,"user":{"displayName":"haobo li","userId":"11080054968865471708"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def data_iter_random(X, x_daily, batch_size, num_steps, device=None):\n","    station_index = [[78, 182], [79, 168], [81, 162], [80, 199], [120, 154], [96, 202], [101, 173], [195, 154],\n","                     [130, 181], [105, 169], [59, 170], [171, 171], [182, 270], [98, 221], [128, 146], [137, 78],\n","                     [83, 60], [168, 100]]\n","    station_ind = [i[0] * 305 + i[1] for i in station_index]\n","    num_examples = (len(X) - 720 - 24)\n","    #     print('examples', num_examples)\n","    epoch_size = num_examples // batch_size\n","    #     print('epoch', epoch_size)\n","    example_indices = list(range(num_examples))\n","\n","    #     print(example_indices)\n","    # random.shuffle(example_indices)\n","\n","    def _data(pos, data, loc):\n","        pos += 720\n","        lon = int((loc//5) % 305)\n","        lat = int((loc//5) // 305 * 5)\n","        if data == 'x':\n","            #             print(pos, pos+num_steps)\n","            #             print(data[pos:pos + num_steps, :, :, :].size())\n","\n","            tmp = x_daily[pos//24-30:pos//24-3, :, lat-10:lat+10, lon-10:lon+10]\n","            return torch.cat((tmp, X[pos - 72:pos, :, lat-10:lat+10, lon-10:lon+10]), 0)\n","        if data == 'y':\n","            #             print(pos)\n","            return X[pos + 24, :, lat, lon]\n","\n","    if device is None:\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    t_s = time.time()\n","    for i in range(epoch_size):\n","        print(i, time.time()-t_s)\n","        t_s = time.time()\n","\n","        # for j in range(0, 73200):\n","        for j in station_ind:\n","            # if j not in station_ind:\n","            #     continue\n","            lon = int(j % 305)\n","            lat = int(j // 305)\n","            # if lon < 20 or lon > 305-20:\n","            #     continue\n","            # if lat < 20 or lat > 240-20:\n","            #     continue\n","\n","            # 每次读取batch_size个随机样本\n","            ii = i * batch_size\n","            batch_indices = example_indices[ii: ii + batch_size]\n","            #         print(batch_indices)\n","            XX = [_data(index, 'x', j) for index in batch_indices]\n","            YY = [_data(index, 'y', j) for index in batch_indices]\n","            XX2 = torch.tensor([[lat, lon, (i+tmp)//30+1, (i+tmp) % 24] for tmp in range(batch_size)])\n","            XX = torch.stack(XX)\n","            YY = torch.stack(YY)\n","            XX = XX.transpose(1, 2)\n","            yield XX, YY, XX2.float()\n"],"metadata":{"id":"hgldI6JXJ_cp","executionInfo":{"status":"ok","timestamp":1652948007461,"user_tz":-480,"elapsed":5,"user":{"displayName":"haobo li","userId":"11080054968865471708"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def train_ch5(net ,net2, net3, batch_size, optimizer, device, num_epochs):\n","    global X\n","    net = net.to(device)\n","    net2 = net2.to(device)\n","    net3 = net3.to(device)\n","    print(\"training on \", device)\n","    loss = torch.nn.MSELoss(reduction='mean')\n","    batch_count = 0\n","    L = []\n","    for epoch in range(num_epochs):\n","        train_l_sum, start = 0.0, time.time()\n","        train_iter = data_iter_random(X[:, :, :, :], x_daily, batch_size, 99)\n","        for x, y, x2 in train_iter:\n","            x = x.to(device)\n","            x2 = x2.to(device)\n","            # X = X.to(device)\n","            y = y.to(device).view(-1)\n","            y1 = net(x)\n","            y2 = net2(x2)\n","            y_hat = net3(torch.cat((y1, y2.view(-1, 1)), 1))\n","            y_hat = y_hat.view(y_hat.shape[0])\n","            # print(y.size())\n","            # print(y_hat.size())\n","\n","            l = loss(y_hat, y)\n","            optimizer.zero_grad()\n","            l.backward()\n","            optimizer.step()\n","            train_l_sum += l.cpu().item()\n","            batch_count += 1\n","        # print(y_hat, y, l)\n","        L.append(train_l_sum / batch_count / batch_size)\n","        print('epoch %d, loss %.4f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / batch_count, time.time() - start))\n","        # x_test = X[1200:1230, :, :, :]\n","        # x_test = x_test.transpose(0, 1)\n","        # x_test = x_test.view(1, 3, 30, 57, 57)\n","        # print(net(x_test))\n","        # x_test = X[0:30, :, :, :]\n","        # x_test = x_test.transpose(0, 1)\n","        # x_test = x_test.view(1, 3, 30, 57, 57)\n","        # print(net(x_test))\n","        torch.save({'epoch': epoch,\n","                    'model_state_dict': net.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': L},\n","                   'model.pt')\n"],"metadata":{"id":"C9aYYqiCKDE0","executionInfo":{"status":"ok","timestamp":1652948008503,"user_tz":-480,"elapsed":2,"user":{"displayName":"haobo li","userId":"11080054968865471708"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["global X, Y, x_daily\n","X = torch.load(\"/content/drive/Othercomputers/我的 MacBook Pro/PycharmProjects/ENV/NO2/data_2019.pt\").float()[:, :]\n","X = X.view(-1, 1, 240, 305)\n","for i in range(len(X) // 24 - 1):\n","    if i == 0:\n","        x_daily = X[i * 24:(i + 1) * 24, :, :, :].mean(0, True)\n","    else:\n","        tmp = X[i * 24:(i + 1) * 24, :, :, :].mean(0, True)\n","        x_daily = torch.cat((x_daily, tmp), 0)\n","\n","mode = 0\n","if mode:\n","    lr, num_epochs = 0.00001, 500\n","    # optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","    batch_size = 1\n","    device = 'cuda'\n","    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n","    checkpoint = torch.load('/Volumes/OS/stock/model2.pt')\n","    net.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    l = checkpoint['loss']\n","    e = checkpoint['epoch']\n","    net.train()\n","else:\n","    lr, num_epochs = 0.00001, 500\n","    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","    batch_size = 256\n","    device = 'cpu'\n","\n","train_ch5(net, net2, net3, batch_size, optimizer, device, num_epochs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9E06IYxKJUx","outputId":"f587a8e0-03cc-4728-ab89-1f05c0d5ebd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["training on  cpu\n","0 1.6689300537109375e-06\n","1 156.9850516319275\n"]}]},{"cell_type":"code","source":[" "],"metadata":{"id":"x4ooyPkWNc9S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NWBQ_h1A-LEy"},"execution_count":null,"outputs":[]}]}